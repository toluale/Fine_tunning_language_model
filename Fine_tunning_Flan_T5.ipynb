{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuntasirHossain/flan-t5-large-samsum-qlora-merged\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "#models = list(api.list_models())\n",
    "\n",
    "models = api.list_models(\n",
    "    #filter=ModelFilter(task=\"text-classification\"),\n",
    "    filter={\"pipeline_tag\": \"text-summarization\"},\n",
    "        sort=\"downloads\",\n",
    "        direction=-1,\n",
    "        limit=5\n",
    "    )\n",
    "modelList = list(models)\n",
    "print(modelList[0].modelId)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "#Text Summarization\n",
    "dataset_name = \"knkarthick/dialogsum\" #loading the dataset from huggingface\n",
    "dataset = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': Value(dtype='string', id=None), 'dialogue': Value(dtype='string', id=None), 'summary': Value(dtype='string', id=None), 'topic': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset_builder #allows inspection of dataset before downloading\n",
    "\n",
    "data_builder = load_dataset_builder(dataset_name)\n",
    "\n",
    "#print(data_builder.info.description)\n",
    "print(data_builder.info.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************************************\n",
      "Sample  1\n",
      "***************************************************************************************************\n",
      "INPUT DIALOGUE:\n",
      "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
      "#Person2#: Yes, sir...\n",
      "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
      "#Person2#: Yes, sir. Go ahead.\n",
      "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
      "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
      "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
      "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
      "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
      "#Person2#: This applies to internal and external communications.\n",
      "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
      "#Person2#: Is that all?\n",
      "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
      "***************************************************************************************************\n",
      "BASELINE HUMAN SUMMARY:\n",
      "In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.\n",
      "***************************************************************************************************\n",
      "\n",
      "***************************************************************************************************\n",
      "Sample  2\n",
      "***************************************************************************************************\n",
      "INPUT DIALOGUE:\n",
      "#Person1#: Is anybody in?\n",
      "#Person2#: How can I help you?\n",
      "#Person1#: I have a headache.\n",
      "#Person2#: Let me take your temperature with a thermometer.\n",
      "#Person1#: OK.\n",
      "#Person2#: I think you have a small fever.\n",
      "#Person1#: I thought so. I felt dizzy this morning.\n",
      "#Person2#: You should've called in sick! Next time, have either of your parents call the school office.\n",
      "***************************************************************************************************\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person2# finds that #Person1# has a fever and says #Person1# should've called in sick earlier.\n",
      "***************************************************************************************************\n",
      "\n",
      "***************************************************************************************************\n",
      "Sample  3\n",
      "***************************************************************************************************\n",
      "INPUT DIALOGUE:\n",
      "#Person1#: May, do you mind helping me prepare for the picnic?\n",
      "#Person2#: Sure. Have you checked the weather report?\n",
      "#Person1#: Yes. It says it will be sunny all day. No sign of rain at all. This is your father's favorite sausage. Sandwiches for you and Daniel.\n",
      "#Person2#: No, thanks Mom. I'd like some toast and chicken wings.\n",
      "#Person1#: Okay. Please take some fruit salad and crackers for me.\n",
      "#Person2#: Done. Oh, don't forget to take napkins disposable plates, cups and picnic blanket.\n",
      "#Person1#: All set. May, can you help me take all these things to the living room?\n",
      "#Person2#: Yes, madam.\n",
      "#Person1#: Ask Daniel to give you a hand?\n",
      "#Person2#: No, mom, I can manage it by myself. His help just causes more trouble.\n",
      "***************************************************************************************************\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Mom asks May to help to prepare for the picnic and May agrees.\n",
      "***************************************************************************************************\n",
      "\n",
      "***************************************************************************************************\n",
      "Sample  4\n",
      "***************************************************************************************************\n",
      "INPUT DIALOGUE:\n",
      "#Person1#: Taxi!\n",
      "#Person2#: Where will you go, sir?\n",
      "#Person1#: Friendship Hotel.\n",
      "#Person2#: OK, it's not far from here.\n",
      "#Person1#: I have something important to do, can you fast the speed?\n",
      "#Person2#: Sure, I'll try my best. Here we are.\n",
      "#Person1#: It's fast! How much should I pay you?\n",
      "#Person2#: The reading on the meter is 15 yuan.\n",
      "#Person1#: Here's 20 yuan, keep the change.\n",
      "#Person2#: Thank you very much.\n",
      "***************************************************************************************************\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# takes a taxi to the Friendship Hotel for something important.\n",
      "***************************************************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Loading some sample dialogues with their baseline\n",
    "dialogues = [1, 55, 80, 150]\n",
    "\n",
    "dash_line = '*'.join('' for x in range(100))\n",
    "\n",
    "for i, index in enumerate(dialogues):\n",
    "    print(dash_line)\n",
    "    print('Sample ', i + 1)\n",
    "    print(dash_line)\n",
    "    print('INPUT DIALOGUE:')\n",
    "    print(dataset['test'][index]['dialogue'])\n",
    "    print(dash_line)\n",
    "    print('BASELINE HUMAN SUMMARY:')\n",
    "    print(dataset['test'][index]['summary'])\n",
    "    print(dash_line)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Loading FLAN-T5 Base model - text-summarization model\n",
    "\n",
    "model_name='google/flan-t5-base'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True) #FLAN-T5 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 21:45:38.497031: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-25 21:45:38.554269: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-25 21:45:39.451893: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************************************\n",
      "Sample  1\n",
      "***************************************************************************************************\n",
      "INPUT PROMPT:\n",
      "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
      "#Person2#: Yes, sir...\n",
      "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
      "#Person2#: Yes, sir. Go ahead.\n",
      "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
      "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
      "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
      "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
      "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
      "#Person2#: This applies to internal and external communications.\n",
      "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
      "#Person2#: Is that all?\n",
      "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
      "***************************************************************************************************\n",
      "BASELINE HUMAN SUMMARY:\n",
      "In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.\n",
      "***************************************************************************************************\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
      "\n",
      "***************************************************************************************************\n",
      "Sample  2\n",
      "***************************************************************************************************\n",
      "INPUT PROMPT:\n",
      "#Person1#: Is anybody in?\n",
      "#Person2#: How can I help you?\n",
      "#Person1#: I have a headache.\n",
      "#Person2#: Let me take your temperature with a thermometer.\n",
      "#Person1#: OK.\n",
      "#Person2#: I think you have a small fever.\n",
      "#Person1#: I thought so. I felt dizzy this morning.\n",
      "#Person2#: You should've called in sick! Next time, have either of your parents call the school office.\n",
      "***************************************************************************************************\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person2# finds that #Person1# has a fever and says #Person1# should've called in sick earlier.\n",
      "***************************************************************************************************\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      "#Person1#: I have a headache. #Person2#: I think you have a small fever.\n",
      "\n",
      "***************************************************************************************************\n",
      "Sample  3\n",
      "***************************************************************************************************\n",
      "INPUT PROMPT:\n",
      "#Person1#: May, do you mind helping me prepare for the picnic?\n",
      "#Person2#: Sure. Have you checked the weather report?\n",
      "#Person1#: Yes. It says it will be sunny all day. No sign of rain at all. This is your father's favorite sausage. Sandwiches for you and Daniel.\n",
      "#Person2#: No, thanks Mom. I'd like some toast and chicken wings.\n",
      "#Person1#: Okay. Please take some fruit salad and crackers for me.\n",
      "#Person2#: Done. Oh, don't forget to take napkins disposable plates, cups and picnic blanket.\n",
      "#Person1#: All set. May, can you help me take all these things to the living room?\n",
      "#Person2#: Yes, madam.\n",
      "#Person1#: Ask Daniel to give you a hand?\n",
      "#Person2#: No, mom, I can manage it by myself. His help just causes more trouble.\n",
      "***************************************************************************************************\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Mom asks May to help to prepare for the picnic and May agrees.\n",
      "***************************************************************************************************\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      "#Person1#: May, can you help me prepare the picnic?\n",
      "\n",
      "***************************************************************************************************\n",
      "Sample  4\n",
      "***************************************************************************************************\n",
      "INPUT PROMPT:\n",
      "#Person1#: Taxi!\n",
      "#Person2#: Where will you go, sir?\n",
      "#Person1#: Friendship Hotel.\n",
      "#Person2#: OK, it's not far from here.\n",
      "#Person1#: I have something important to do, can you fast the speed?\n",
      "#Person2#: Sure, I'll try my best. Here we are.\n",
      "#Person1#: It's fast! How much should I pay you?\n",
      "#Person2#: The reading on the meter is 15 yuan.\n",
      "#Person1#: Here's 20 yuan, keep the change.\n",
      "#Person2#: Thank you very much.\n",
      "***************************************************************************************************\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# takes a taxi to the Friendship Hotel for something important.\n",
      "***************************************************************************************************\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      "The taxi driver will pick you up at the Friendship Hotel at 20 yuan.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Base LLM summarization without prompt engineering\n",
    "for i, index in enumerate(dialogues):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "    \n",
    "    inputs = tokenizer(dialogue, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=50,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(dash_line)\n",
    "    print('Sample ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models performance was poor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************************************\n",
      "Sample  1\n",
      "***************************************************************************************************\n",
      "INPUT PROMPT:\n",
      "Can you read and summarize the conversation?\n",
      "\n",
      "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
      "#Person2#: Yes, sir...\n",
      "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
      "#Person2#: Yes, sir. Go ahead.\n",
      "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
      "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
      "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
      "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
      "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
      "#Person2#: This applies to internal and external communications.\n",
      "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
      "#Person2#: Is that all?\n",
      "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
      "\n",
      "Summary:\n",
      "    \n",
      "***************************************************************************************************\n",
      "BASELINE HUMAN SUMMARY:\n",
      "In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.\n",
      "***************************************************************************************************\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Employees are required to use instant messaging in all communications.\n",
      "\n",
      "***************************************************************************************************\n",
      "Sample  2\n",
      "***************************************************************************************************\n",
      "INPUT PROMPT:\n",
      "Can you read and summarize the conversation?\n",
      "\n",
      "#Person1#: Is anybody in?\n",
      "#Person2#: How can I help you?\n",
      "#Person1#: I have a headache.\n",
      "#Person2#: Let me take your temperature with a thermometer.\n",
      "#Person1#: OK.\n",
      "#Person2#: I think you have a small fever.\n",
      "#Person1#: I thought so. I felt dizzy this morning.\n",
      "#Person2#: You should've called in sick! Next time, have either of your parents call the school office.\n",
      "\n",
      "Summary:\n",
      "    \n",
      "***************************************************************************************************\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person2# finds that #Person1# has a fever and says #Person1# should've called in sick earlier.\n",
      "***************************************************************************************************\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "#Person1: I have a headache. #Person2: I think you have a small fever.\n",
      "\n",
      "***************************************************************************************************\n",
      "Sample  3\n",
      "***************************************************************************************************\n",
      "INPUT PROMPT:\n",
      "Can you read and summarize the conversation?\n",
      "\n",
      "#Person1#: May, do you mind helping me prepare for the picnic?\n",
      "#Person2#: Sure. Have you checked the weather report?\n",
      "#Person1#: Yes. It says it will be sunny all day. No sign of rain at all. This is your father's favorite sausage. Sandwiches for you and Daniel.\n",
      "#Person2#: No, thanks Mom. I'd like some toast and chicken wings.\n",
      "#Person1#: Okay. Please take some fruit salad and crackers for me.\n",
      "#Person2#: Done. Oh, don't forget to take napkins disposable plates, cups and picnic blanket.\n",
      "#Person1#: All set. May, can you help me take all these things to the living room?\n",
      "#Person2#: Yes, madam.\n",
      "#Person1#: Ask Daniel to give you a hand?\n",
      "#Person2#: No, mom, I can manage it by myself. His help just causes more trouble.\n",
      "\n",
      "Summary:\n",
      "    \n",
      "***************************************************************************************************\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Mom asks May to help to prepare for the picnic and May agrees.\n",
      "***************************************************************************************************\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "May will help prepare the food for the picnic.\n",
      "\n",
      "***************************************************************************************************\n",
      "Sample  4\n",
      "***************************************************************************************************\n",
      "INPUT PROMPT:\n",
      "Can you read and summarize the conversation?\n",
      "\n",
      "#Person1#: Taxi!\n",
      "#Person2#: Where will you go, sir?\n",
      "#Person1#: Friendship Hotel.\n",
      "#Person2#: OK, it's not far from here.\n",
      "#Person1#: I have something important to do, can you fast the speed?\n",
      "#Person2#: Sure, I'll try my best. Here we are.\n",
      "#Person1#: It's fast! How much should I pay you?\n",
      "#Person2#: The reading on the meter is 15 yuan.\n",
      "#Person1#: Here's 20 yuan, keep the change.\n",
      "#Person2#: Thank you very much.\n",
      "\n",
      "Summary:\n",
      "    \n",
      "***************************************************************************************************\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# takes a taxi to the Friendship Hotel for something important.\n",
      "***************************************************************************************************\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "The taxi driver will pick up Person1 at Friendship Hotel at 20 yuan.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using instruction prompt\n",
    "# Zero Shot Inference: wrap the dialogue in a descriptive instruction\n",
    "\n",
    "for i, index in enumerate(dialogues):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "\n",
    "    prompt = f\"\"\"Can you read and summarize the conversation?\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "\n",
    "    # Input constructed prompt instead of the dialogue.\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=50,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(dash_line)\n",
    "    print('Sample ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{prompt}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)    \n",
    "    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can attempt different prompt instruction to see if the model summary improves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************************************\n",
      "Example  1\n",
      "***************************************************************************************************\n",
      "INPUT PROMPT:\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
      "#Person2#: Yes, sir...\n",
      "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
      "#Person2#: Yes, sir. Go ahead.\n",
      "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
      "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
      "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
      "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
      "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
      "#Person2#: This applies to internal and external communications.\n",
      "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
      "#Person2#: Is that all?\n",
      "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
      "\n",
      "What was going on?\n",
      "\n",
      "***************************************************************************************************\n",
      "BASELINE HUMAN SUMMARY:\n",
      "In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.\n",
      "\n",
      "***************************************************************************************************\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "The memo is to be distributed to all employees by this afternoon.\n",
      "\n",
      "***************************************************************************************************\n",
      "Example  2\n",
      "***************************************************************************************************\n",
      "INPUT PROMPT:\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Is anybody in?\n",
      "#Person2#: How can I help you?\n",
      "#Person1#: I have a headache.\n",
      "#Person2#: Let me take your temperature with a thermometer.\n",
      "#Person1#: OK.\n",
      "#Person2#: I think you have a small fever.\n",
      "#Person1#: I thought so. I felt dizzy this morning.\n",
      "#Person2#: You should've called in sick! Next time, have either of your parents call the school office.\n",
      "\n",
      "What was going on?\n",
      "\n",
      "***************************************************************************************************\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person2# finds that #Person1# has a fever and says #Person1# should've called in sick earlier.\n",
      "\n",
      "***************************************************************************************************\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Person1 has a headache and feels dizzy.\n",
      "\n",
      "***************************************************************************************************\n",
      "Example  3\n",
      "***************************************************************************************************\n",
      "INPUT PROMPT:\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: May, do you mind helping me prepare for the picnic?\n",
      "#Person2#: Sure. Have you checked the weather report?\n",
      "#Person1#: Yes. It says it will be sunny all day. No sign of rain at all. This is your father's favorite sausage. Sandwiches for you and Daniel.\n",
      "#Person2#: No, thanks Mom. I'd like some toast and chicken wings.\n",
      "#Person1#: Okay. Please take some fruit salad and crackers for me.\n",
      "#Person2#: Done. Oh, don't forget to take napkins disposable plates, cups and picnic blanket.\n",
      "#Person1#: All set. May, can you help me take all these things to the living room?\n",
      "#Person2#: Yes, madam.\n",
      "#Person1#: Ask Daniel to give you a hand?\n",
      "#Person2#: No, mom, I can manage it by myself. His help just causes more trouble.\n",
      "\n",
      "What was going on?\n",
      "\n",
      "***************************************************************************************************\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Mom asks May to help to prepare for the picnic and May agrees.\n",
      "\n",
      "***************************************************************************************************\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Person1 wants to prepare for the picnic. May will help her.\n",
      "\n",
      "***************************************************************************************************\n",
      "Example  4\n",
      "***************************************************************************************************\n",
      "INPUT PROMPT:\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Taxi!\n",
      "#Person2#: Where will you go, sir?\n",
      "#Person1#: Friendship Hotel.\n",
      "#Person2#: OK, it's not far from here.\n",
      "#Person1#: I have something important to do, can you fast the speed?\n",
      "#Person2#: Sure, I'll try my best. Here we are.\n",
      "#Person1#: It's fast! How much should I pay you?\n",
      "#Person2#: The reading on the meter is 15 yuan.\n",
      "#Person1#: Here's 20 yuan, keep the change.\n",
      "#Person2#: Thank you very much.\n",
      "\n",
      "What was going on?\n",
      "\n",
      "***************************************************************************************************\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# takes a taxi to the Friendship Hotel for something important.\n",
      "\n",
      "***************************************************************************************************\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "The taxi driver will pick up Person1 at Friendship Hotel at 20 yuan.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using a prompt template. The dialogue were passed into the model as prompt before the instruction\n",
    "\n",
    "for i, index in enumerate(dialogues):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "        \n",
    "    prompt = f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=50,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{prompt}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Yeah. Just pull on this strip. Then peel off the back.\n",
      "#Person2#: You might make a few enemies this way.\n",
      "#Person1#: If they don't think this is fun, they're not meant to be our friends.\n",
      "#Person2#: You mean your friends. I think it's cruel.\n",
      "#Person1#: Yeah. But it's fun. Look at those two ugly old ladies. . . or are they men?\n",
      "#Person2#: Hurry! Get a shot!. . . Hand it over!\n",
      "#Person1#: I knew you'd come around. . .\n",
      "\n",
      "What was going on?\n",
      "#Person2# at first thinks #Person1#'s behaviour cruel but finally joins #Person1#.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Taxi!\n",
      "#Person2#: Where will you go, sir?\n",
      "#Person1#: Friendship Hotel.\n",
      "#Person2#: OK, it's not far from here.\n",
      "#Person1#: I have something important to do, can you fast the speed?\n",
      "#Person2#: Sure, I'll try my best. Here we are.\n",
      "#Person1#: It's fast! How much should I pay you?\n",
      "#Person2#: The reading on the meter is 15 yuan.\n",
      "#Person1#: Here's 20 yuan, keep the change.\n",
      "#Person2#: Thank you very much.\n",
      "\n",
      "What was going on?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Trying One Shot and Few Shot Inference to improve the model's generation\n",
    "#We will be giving the model some example dialogue and the baseline summary before introducing the task\n",
    "\n",
    "##One-Shot Inference\n",
    "\n",
    "def make_prompt(indices_full, index_to_summarize):\n",
    "    prompt = ''\n",
    "    for index in indices_full:\n",
    "        dialogue = dataset['test'][index]['dialogue']\n",
    "        summary = dataset['test'][index]['summary']\n",
    "        \n",
    "        # The stop sequence '{summary}\\n\\n\\n' is important for FLAN-T5. \n",
    "        # Other models may have their own preferred stop sequence.\n",
    "        prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "{summary}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    dialogue = dataset['test'][index_to_summarize]['dialogue']\n",
    "    \n",
    "    prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "\"\"\"\n",
    "        \n",
    "    return prompt\n",
    "\n",
    "indices_full = [48]\n",
    "index_to_summarize = 150\n",
    "\n",
    "one_shot_prompt = make_prompt(indices_full, index_to_summarize)\n",
    "\n",
    "print(one_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************************************\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# takes a taxi to the Friendship Hotel for something important.\n",
      "\n",
      "***************************************************************************************************\n",
      "MODEL GENERATION - ONE SHOT:\n",
      "The taxi driver will take Person1 to Friendship Hotel.\n"
     ]
    }
   ],
   "source": [
    "summary = dataset['test'][index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=50,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ONE SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Yeah. Just pull on this strip. Then peel off the back.\n",
      "#Person2#: You might make a few enemies this way.\n",
      "#Person1#: If they don't think this is fun, they're not meant to be our friends.\n",
      "#Person2#: You mean your friends. I think it's cruel.\n",
      "#Person1#: Yeah. But it's fun. Look at those two ugly old ladies. . . or are they men?\n",
      "#Person2#: Hurry! Get a shot!. . . Hand it over!\n",
      "#Person1#: I knew you'd come around. . .\n",
      "\n",
      "What was going on?\n",
      "#Person2# at first thinks #Person1#'s behaviour cruel but finally joins #Person1#.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: What does your sister look like, Mike?\n",
      "#Person2#: Well, she's tall and pretty.\n",
      "#Person1#: Is she like you?\n",
      "#Person2#: I suppose so. We're both friendly and easy-going.\n",
      "#Person1#: Is she as clever as you?\n",
      "#Person2#: No, she's not as clever as me.\n",
      "#Person1#: Big head!\n",
      "\n",
      "What was going on?\n",
      "Mike is describing his sister to #Person1#.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Is this the workshop to prepare for an interview?\n",
      "#Person2#: This is the interview class. Welcome to our class.\n",
      "#Person1#: I am really excited to be taking this workshop so that I can get ready for my interview next week.\n",
      "#Person2#: We are all learning things that will help us in our interview. What do you think are some important considerations going into your interview?\n",
      "#Person1#: I think that we should dress neatly and appropriately.\n",
      "#Person2#: Yes. Second, as you can imagine, attitude and friendliness go a long way.\n",
      "#Person1#: Yes, and I always feel much better when I am friendly.\n",
      "#Person2#: Believe it or not, the interviewers are as interested in your questions as they are in your answers.\n",
      "#Person1#: Any more hints as to what I should do in an interview?\n",
      "#Person2#: Always be honest with your answers. The interviewers really do want to know if you will be a good fit for them.\n",
      "\n",
      "What was going on?\n",
      "In the workshop, #Person2# offer #Person1# some suggestions on how to perform well in interviews.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Well, I'll see you later, Mrs. Todd. My wife is waiting for me to take her shopping.\n",
      "#Person2#: I understand. There's a lot to get done at weekends, especially when you two work and the children are small.\n",
      "#Person1#: That's right. Jane and I have been talking about visiting you. So when I saw you in the garden, I decided to come over and say hello.\n",
      "#Person2#: I'm glad you did. In fact, I should have called on you first, since you have newly moved here.\n",
      "#Person1#: By the way, do you need anything from the store?\n",
      "#Person2#: No, but thanks for the offer. And thank you for coming over.\n",
      "#Person1#: It's a pleasure.\n",
      "\n",
      "What was going on?\n",
      "#Person1# greets Mrs. Todd and then they say goodbye to each other.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Taxi!\n",
      "#Person2#: Where will you go, sir?\n",
      "#Person1#: Friendship Hotel.\n",
      "#Person2#: OK, it's not far from here.\n",
      "#Person1#: I have something important to do, can you fast the speed?\n",
      "#Person2#: Sure, I'll try my best. Here we are.\n",
      "#Person1#: It's fast! How much should I pay you?\n",
      "#Person2#: The reading on the meter is 15 yuan.\n",
      "#Person1#: Here's 20 yuan, keep the change.\n",
      "#Person2#: Thank you very much.\n",
      "\n",
      "What was going on?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Few-Shot Inference\n",
    "\n",
    "indices_full = [48, 51, 98, 102]\n",
    "index_to_summarize = 150\n",
    "\n",
    "few_shot_prompt = make_prompt(indices_full, index_to_summarize)\n",
    "\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (956 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************************************\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# takes a taxi to the Friendship Hotel for something important.\n",
      "\n",
      "***************************************************************************************************\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "The taxi driver will pick up Person1 at Friendship Hotel at 20 yuan.\n"
     ]
    }
   ],
   "source": [
    "#passing the prompts into the model to perform few shot propmting\n",
    "\n",
    "summary = dataset['test'][index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=50,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimenting with different number of input prompt shows no significant result from one-shot inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use some configuration parameters to influence the output\n",
    "available parameters can be found in [huggingface documentation](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************************************\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# takes a taxi to the Friendship Hotel for something important.\n",
      "\n",
      "***************************************************************************************************\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "The taxi meter reading is 15 yuan, so the driver will have to give 20 yuan.\n"
     ]
    }
   ],
   "source": [
    "# Temperature controls the randomness inthe output\n",
    "generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.70)\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        generation_config=generation_config,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fine-Tunning our model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# We will reload the FLAN-T5 base model but the small version due to compute \n",
    "# by setting torch_dtype=torch.bfloat16\n",
    "\n",
    "model_name='google/flan-t5-base'\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 247577856\n",
      "all model parameters: 247577856\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Examining the number of trainable model parameters\n",
    "\n",
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset by converting the dialog-summary (prompt-reponse) pairs into explicit instructions\n",
    "# The convert the prompt-response into tokens and pull out their input_ids (1 per token)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    return example\n",
    "\n",
    "# The dataset actually contains 3 diff splits: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (12460, 2)\n",
      "Validation: (500, 2)\n",
      "Test: (1500, 2)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 12460\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 1500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We train the model using the hugging face trainer class. \n",
    "# pass the preprocessed dataset with reference to the originalmodel\n",
    "\n",
    "output_dir = f'/home/tale2@ad.umbc.edu/LLM/text_summarization/flan-t5-finetuned-{str(int(time.time()))}'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    save_steps=30,\n",
    "    max_steps=2000,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    bf16=True,   \n",
    "    fp16=False  \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=original_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    #tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1930' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1930/2000 40:38:34 < 1:28:32, 0.01 it/s, Epoch 4.95/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>10.618900</td>\n",
       "      <td>4.994411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.850500</td>\n",
       "      <td>4.309980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>4.192200</td>\n",
       "      <td>3.424272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.551100</td>\n",
       "      <td>2.381094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.002300</td>\n",
       "      <td>1.845306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.614000</td>\n",
       "      <td>1.398491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.309700</td>\n",
       "      <td>1.112099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.059000</td>\n",
       "      <td>0.897564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.864700</td>\n",
       "      <td>0.762956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.663500</td>\n",
       "      <td>0.661860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.545800</td>\n",
       "      <td>0.601209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.458400</td>\n",
       "      <td>0.562503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.349800</td>\n",
       "      <td>0.529196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.282100</td>\n",
       "      <td>0.505091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.255300</td>\n",
       "      <td>0.480771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.194900</td>\n",
       "      <td>0.467012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.164900</td>\n",
       "      <td>0.455468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.146800</td>\n",
       "      <td>0.447007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.120400</td>\n",
       "      <td>0.439340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.097200</td>\n",
       "      <td>0.434848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.113700</td>\n",
       "      <td>0.429760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.086400</td>\n",
       "      <td>0.427951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.084400</td>\n",
       "      <td>0.427207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.075900</td>\n",
       "      <td>0.426014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.059700</td>\n",
       "      <td>0.424898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.061900</td>\n",
       "      <td>0.425200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.051800</td>\n",
       "      <td>0.423025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.074000</td>\n",
       "      <td>0.422678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.057900</td>\n",
       "      <td>0.422181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.084100</td>\n",
       "      <td>0.421813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.048300</td>\n",
       "      <td>0.421174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.043600</td>\n",
       "      <td>0.420377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.059500</td>\n",
       "      <td>0.420162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.037800</td>\n",
       "      <td>0.420066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.066000</td>\n",
       "      <td>0.419893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.072600</td>\n",
       "      <td>0.419874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>1.060100</td>\n",
       "      <td>0.419859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.066500</td>\n",
       "      <td>0.420031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/tale2@ad.umbc.edu/myenv3.10/lib/python3.10/site-packages/transformers/trainer.py:2608: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/tale2@ad.umbc.edu/LLM/text_summarization/flan-t5-finetuned-1743735222\"\n",
    "fine_tuned_model = AutoModelForSeq2SeqLM.from_pretrained(model_path, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************************************\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# takes a taxi to the Friendship Hotel for something important.\n",
      "***************************************************************************************************\n",
      "ORIGINAL MODEL:\n",
      "The taxi will pick you up at the Friendship Hotel at 20 yuan.\n",
      "***************************************************************************************************\n",
      "FINE-TUNE MODEL:\n",
      "The taxi will pick you up at the Friendship Hotel at 20 yuan.\n"
     ]
    }
   ],
   "source": [
    "# Qualitative evaluation\n",
    "\n",
    "index = 150\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "human_baseline_summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "fine_tuned_model_outputs = fine_tuned_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "fine_tuned_model_text_output = tokenizer.decode(fine_tuned_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'FINE-TUNE MODEL:\\n{fine_tuned_model_text_output}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
